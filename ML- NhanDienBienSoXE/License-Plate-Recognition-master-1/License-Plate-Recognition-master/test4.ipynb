{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cv2 import threshold\n",
    "import src.data_utils as utils\n",
    "import cv2\n",
    "import numpy as np\n",
    "from imutils import perspective\n",
    "from src.data_utils import order_points, convert2Square, draw_labels_and_boxes\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage import measure\n",
    "from imutils import perspective\n",
    "import imutils\n",
    "from src.char_classification.model import CNN_Model\n",
    "from skimage.filters import threshold_local\n",
    "from imutils.contours import sort_contours\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_mapping = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890'\n",
    "# class_mapping = '1234567890ABCDEFGHIJKLMNOPQRSTUVWXYZabdefghnqrt'\n",
    "# class_mapping = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'\n",
    "class_mapping = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890@#$%&(){}[],?'\n",
    "# class_mapping = '0123456789'\n",
    "labelNames = [l for l in class_mapping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:'a',1:'b',2:'c',3:'d',4:'e',5:'f',6:'g',7:'h',8:'i',9:'j',10:'k',11:'l',12:'m',13:'n',14:'o',15:'p',16:'q',17:'r',18:'s',19:'t',20:'u',21:'v',22:'w',23:'x',24:'y',25:'z',26:'A',27:'B',28:'C',29:'D',30:'E',31:'F',32:'G',33:'H',34:'I',35:'J',36:'K',37:'L',38:'M',39:'N',40:'O',41:'P',42:'Q',43:'R',44:'S',45:'T',46:'U',47:'V',48:'W',49:'X',50:'Y',51:'Z',52:'1',53:'2',54:'3',55:'4',56:'5',57:'6',58:'7',59:'8',60:'9',61:'0',\n"
     ]
    }
   ],
   "source": [
    "s=\"\"\n",
    "i=0\n",
    "for c in labelNames:\n",
    "   s+=str(i)+\":\\'\"+c+\"\\',\" \n",
    "   i+=1\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labelNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(316, 400)\n"
     ]
    }
   ],
   "source": [
    "boxes = []\n",
    "classes_id = []\n",
    "confidences = []\n",
    "scale = 0.00392\n",
    "CHAR_CLASSIFICATION_WEIGHTS = './src/weights/bestmodel2-9.hdf5'\n",
    "image = cv2.imread(r\"C:\\Users\\hoang\\Downloads\\License-Plate-Recognition-master\\samples\\10.jpg\")\n",
    "blob = cv2.dnn.blobFromImage(image, scalefactor=scale, size=(416, 416), mean=(0, 0), swapRB=True, crop=False)\n",
    "height, width = image.shape[:2]\n",
    "threshold=0.5\n",
    "# recogChar = CNN_Model(trainable=False).model\n",
    "# recogChar.load_weights(CHAR_CLASSIFICATION_WEIGHTS)\n",
    "recogChar=model=keras.models.load_model(\"./src/weights/bestmodel2-9.hdf5\")\n",
    "LP_DETECTION_CFG = {\n",
    "    \"weight_path\": \"./src/weights/yolov3-tiny_15000.weights\",\n",
    "    \"classes_path\": \"./src/lp_detection/cfg/yolo.names\",\n",
    "    \"config_path\": \"./src/lp_detection/cfg/yolov3-tiny.cfg\"\n",
    "}\n",
    "# take image to model\n",
    "LP_DETECTION_CFG['classes_path']\n",
    "model = cv2.dnn.readNet(model=LP_DETECTION_CFG['weight_path'], config=LP_DETECTION_CFG['config_path'])\n",
    "model.setInput(blob)\n",
    "# run forward\n",
    "outputs = model.forward(utils.get_output_layers(model))\n",
    "\n",
    "for output in outputs:\n",
    "    for i in range(len(output)):\n",
    "        scores = output[i][5:]\n",
    "        class_id = np.argmax(scores)\n",
    "        confidence = float(scores[class_id])\n",
    "\n",
    "        if confidence > threshold:\n",
    "            # coordinate of bounding boxes\n",
    "            center_x = int(output[i][0] * width)\n",
    "            center_y = int(output[i][1] * height)\n",
    "\n",
    "            detected_width = int(output[i][2] * width)\n",
    "            detected_height = int(output[i][3] * height)\n",
    "\n",
    "            x_min = center_x - detected_width / 2\n",
    "            y_min = center_y - detected_height / 2\n",
    "\n",
    "            boxes.append([x_min, y_min, detected_width, detected_height])\n",
    "            classes_id.append(class_id)\n",
    "            confidences.append(confidence)\n",
    "\n",
    "indices = cv2.dnn.NMSBoxes(boxes, confidences, score_threshold=threshold, nms_threshold=0.4)\n",
    "\n",
    "coordinates = []\n",
    "for i in indices:\n",
    "    index = i\n",
    "    x_min, y_min, width, height = boxes[index]\n",
    "    x_min = round(x_min)\n",
    "    y_min = round(y_min)\n",
    "\n",
    "    coordinates.append((x_min, y_min, width, height))\n",
    "for coordinate in coordinates:\n",
    "    candidates=[]\n",
    "    pts = order_points(coordinate)\n",
    "\n",
    "            # crop number plate used by bird's eyes view transformation\n",
    "    LpRegion = perspective.four_point_transform(image, pts)\n",
    "    gray = cv2.cvtColor(LpRegion,cv2.COLOR_BGR2GRAY)\n",
    "    # cv2.imshow(\"gray\",gray)\n",
    "    # cv2.waitKey(0)\n",
    "    # image = cv2.imread(r\"C:\\Users\\hoang\\Downloads\\License-Plate-Recognition-master\\samples\\1-2.jpg\")\n",
    "    # gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "            # crop number plate used by bird's eyes view transformation\n",
    "# LpRegion = perspective.four_point_transform(\"C:\\Users\\hoang\\Downloads\\License-Plate-Recognition-master\\samples\\1.jpg\", pts)\n",
    "    V = cv2.split(cv2.cvtColor(LpRegion, cv2.COLOR_BGR2HSV))[2]\n",
    "\n",
    "        # adaptive threshold\n",
    "    T = threshold_local(V, 15, offset=10, method=\"gaussian\")\n",
    "    thresh = (V > T).astype(\"uint8\") * 255\n",
    "# cv2.imshow(\"thresh\",thresh)\n",
    "# cv2.waitKey(0)\n",
    "        # convert black pixel of digits to white pixel\n",
    "    thresh = cv2.bitwise_not(thresh)\n",
    "    thresh = imutils.resize(thresh, width=400)\n",
    "    thresh = cv2.medianBlur(thresh, 5)\n",
    "    # cv2.imshow(\"thresh\",thresh)\n",
    "    # cv2.waitKey(0)\n",
    "        # connected components analysis\n",
    "    labels = measure.label(thresh, connectivity=2, background=0)\n",
    "    print(labels.shape)\n",
    "        # loop over the unique components\n",
    "    for label in np.unique(labels):\n",
    "            # if this is background label, ignore it\n",
    "        if label == 0:\n",
    "            continue\n",
    "\n",
    "            # init mask to store the location of the character candidates\n",
    "        mask = np.zeros(thresh.shape, dtype=\"uint8\")\n",
    "        mask[labels == label] = 255\n",
    "\n",
    "            # find contours from mask\n",
    "        contours, hierarchy = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        # sorted_ctrs = sorted(contours, key=lambda contours: cv2.boundingRect(contours)[0] + cv2.boundingRect(contours)[1] * thresh.shape[1] )\n",
    "        if len(contours) > 0:\n",
    "            contour = max(contours, key=cv2.contourArea)\n",
    "            (x, y, w, h) = cv2.boundingRect(contour)\n",
    "            candidate = np.array(mask[y:y + h, x:x + w])\n",
    "        # cv2.imshow(\"roi\",roi)\n",
    "        # cv2.waitKey(0)\n",
    "                # rule to determine characters\n",
    "            aspectRatio = w / float(h)\n",
    "            solidity = cv2.contourArea(contour) / float(w * h)\n",
    "            heightRatio = h / float(gray.shape[0])\n",
    "\n",
    "            if 0.1 < aspectRatio < 1.0 and solidity > 0.1 and 0.35 < heightRatio < 2.0:\n",
    "                    # extract characters\n",
    "                candidate = np.array(mask[y:y + h, x:x + w])\n",
    "                # cv2.imshow(\"roi\",candidate)\n",
    "                # cv2.waitKey(0)\n",
    "                square_candidate = convert2Square(candidate)\n",
    "                square_candidate = cv2.resize(square_candidate, (60, 60), cv2.INTER_AREA)\n",
    "                square_candidate = square_candidate.reshape((60, 60, 1))\n",
    "                # cv2.imshow(\"roi\",square_candidate)\n",
    "                # cv2.waitKey(0)\n",
    "                candidates.append((square_candidate, (y, x)))\n",
    "    characters = []\n",
    "    coordinates = []\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recogChar=keras.models.load_model(\"./src/weights/yolov3-tiny_15000.weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recogChar.predict_on_batch(candidates[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for char, coordinate in candidates:\n",
    "        # preds=np.argmax(recogChar.predict(char/255))\n",
    "        # label = labelNames[preds]\n",
    "        # print(preds)\n",
    "        # cv2.imshow(\"char\",char)\n",
    "        # cv2.waitKey(0)\n",
    "        characters.append(char)\n",
    "        coordinates.append(coordinate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 60, 1)\n"
     ]
    }
   ],
   "source": [
    "chars = (np.array(characters))/255\n",
    "print(chars[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "recogChar=keras.models.load_model(\"./src/weights/weight.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 30ms/step\n",
      "[52 19 66 74  9 50 62  9 74]\n"
     ]
    }
   ],
   "source": [
    "preds=np.argmax(recogChar.predict(chars),axis=1)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "t\n",
      "&\n",
      "?\n",
      "j\n",
      "Y\n",
      "@\n",
      "j\n",
      "?\n",
      "1t&?jY@j?\n"
     ]
    }
   ],
   "source": [
    "w=\"\"\n",
    "i=0\n",
    "for pred in preds:\n",
    "        # cv2.imshow(\"char\",candid/ates[i][0])\n",
    "        i+=1\n",
    "        # cv2.waitKey(0)\n",
    "        label = labelNames[pred]\n",
    "        print(label)\n",
    "        w+=label\n",
    "print(w)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9d2e2287b90f84c05b564773cad156a65fe051f83fa7c81a3ad23c3ed1bf9926"
  },
  "kernelspec": {
   "display_name": "Python 3.9.11 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
